{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d46260b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import operator\n",
    "import os\n",
    "import re\n",
    "from datetime import date, datetime, timedelta\n",
    "from pathlib import Path\n",
    "from typing import TypedDict, List, Optional, Literal, Annotated\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.types import Send\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7964d97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1) Schemas\n",
    "# -----------------------------\n",
    "class Task(BaseModel):\n",
    "    id: int\n",
    "    title: str\n",
    "\n",
    "    goal: str = Field(\n",
    "        ...,\n",
    "        description=\"One sentence describing what the reader should be able to do/understand after this section.\",\n",
    "    )\n",
    "    bullets: List[str] = Field(\n",
    "        ...,\n",
    "        min_length=3,\n",
    "        max_length=6,\n",
    "        description=\"3–6 concrete, non-overlapping subpoints to cover in this section.\",\n",
    "    )\n",
    "    target_words: int = Field(..., description=\"Target word count for this section (120–550).\")\n",
    "\n",
    "    tags: List[str] = Field(default_factory=list)\n",
    "    requires_research: bool = False\n",
    "    requires_citations: bool = False\n",
    "    requires_code: bool = False\n",
    "\n",
    "\n",
    "class Plan(BaseModel):\n",
    "    blog_title: str\n",
    "    audience: str\n",
    "    tone: str\n",
    "\n",
    "    # NEW: tells workers what genre this is (prevents drift)\n",
    "    blog_kind: Literal[\"explainer\", \"tutorial\", \"news_roundup\", \"comparison\", \"system_design\"] = \"explainer\"\n",
    "\n",
    "    constraints: List[str] = Field(default_factory=list)\n",
    "    tasks: List[Task]\n",
    "\n",
    "\n",
    "class EvidenceItem(BaseModel):\n",
    "    title: str\n",
    "    url: str\n",
    "    published_at: Optional[str] = None  # prefer ISO \"YYYY-MM-DD\"\n",
    "    snippet: Optional[str] = None\n",
    "    source: Optional[str] = None\n",
    "\n",
    "\n",
    "class RouterDecision(BaseModel):\n",
    "    needs_research: bool\n",
    "    mode: Literal[\"closed_book\", \"hybrid\", \"open_book\"]\n",
    "    reason: str\n",
    "    queries: List[str] = Field(default_factory=list)\n",
    "    max_results_per_query: int = Field(5, description=\"How many results to fetch per query (3–8).\")\n",
    "\n",
    "\n",
    "class EvidencePack(BaseModel):\n",
    "    evidence: List[EvidenceItem] = Field(default_factory=list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12e00b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    topic: str\n",
    "\n",
    "    # routing / research\n",
    "    mode: str\n",
    "    needs_research: bool\n",
    "    queries: List[str]\n",
    "    evidence: List[EvidenceItem]\n",
    "    plan: Optional[Plan]\n",
    "\n",
    "    # NEW: recency control\n",
    "    as_of: str           # ISO date, e.g. \"2026-01-29\"\n",
    "    recency_days: int    # 7 for weekly news, 30 for hybrid, etc.\n",
    "\n",
    "    # workers\n",
    "    sections: Annotated[List[tuple[int, str]], operator.add]  # (task_id, section_md)\n",
    "    final: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd530b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 2) LLM\n",
    "# -----------------------------\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62e1cc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 3) Router (decide upfront)\n",
    "# -----------------------------\n",
    "ROUTER_SYSTEM = \"\"\"You are a routing module for a technical blog planner.\n",
    "\n",
    "Decide whether web research is needed BEFORE planning.\n",
    "\n",
    "Modes:\n",
    "- closed_book (needs_research=false):\n",
    "  Evergreen topics where correctness does not depend on recent facts (concepts, fundamentals).\n",
    "- hybrid (needs_research=true):\n",
    "  Mostly evergreen but needs up-to-date examples/tools/models to be useful.\n",
    "- open_book (needs_research=true):\n",
    "  Mostly volatile: weekly roundups, \"this week\", \"latest\", rankings, pricing, policy/regulation.\n",
    "\n",
    "If needs_research=true:\n",
    "- Output 3–10 high-signal queries.\n",
    "- Queries should be scoped and specific (avoid generic queries like just \"AI\" or \"LLM\").\n",
    "- For open_book weekly roundup, include queries that reflect the last 7 days constraint.\n",
    "\"\"\"\n",
    "\n",
    "def router_node(state: State) -> dict:\n",
    "    topic = state[\"topic\"]\n",
    "    decider = llm.with_structured_output(RouterDecision)\n",
    "    decision = decider.invoke(\n",
    "        [\n",
    "            SystemMessage(content=ROUTER_SYSTEM),\n",
    "            HumanMessage(content=f\"Topic: {topic}\\nAs-of date: {state['as_of']}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Set default recency window based on mode\n",
    "    if decision.mode == \"open_book\":\n",
    "        recency_days = 7\n",
    "    elif decision.mode == \"hybrid\":\n",
    "        recency_days = 45\n",
    "    else:\n",
    "        recency_days = 3650\n",
    "\n",
    "    return {\n",
    "        \"needs_research\": decision.needs_research,\n",
    "        \"mode\": decision.mode,\n",
    "        \"queries\": decision.queries,\n",
    "        \"recency_days\": recency_days,\n",
    "    }\n",
    "\n",
    "def route_next(state: State) -> str:\n",
    "    return \"research\" if state[\"needs_research\"] else \"orchestrator\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69c69df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 4) Research (Tavily)\n",
    "# -----------------------------\n",
    "def _tavily_search(query: str, max_results: int = 5) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Uses TavilySearchResults if installed and TAVILY_API_KEY is set.\n",
    "    Returns list of dict with common fields. Note: published date is often missing.\n",
    "    \"\"\"\n",
    "    tool = TavilySearchResults(max_results=max_results)\n",
    "    results = tool.invoke({\"query\": query})\n",
    "\n",
    "    normalized: List[dict] = []\n",
    "    for r in results or []:\n",
    "        normalized.append(\n",
    "            {\n",
    "                \"title\": r.get(\"title\") or \"\",\n",
    "                \"url\": r.get(\"url\") or \"\",\n",
    "                \"snippet\": r.get(\"content\") or r.get(\"snippet\") or \"\",\n",
    "                \"published_at\": r.get(\"published_date\") or r.get(\"published_at\"),\n",
    "                \"source\": r.get(\"source\"),\n",
    "            }\n",
    "        )\n",
    "    return normalized\n",
    "\n",
    "\n",
    "def _iso_to_date(s: Optional[str]) -> Optional[date]:\n",
    "    if not s:\n",
    "        return None\n",
    "    try:\n",
    "        return date.fromisoformat(s[:10])\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "RESEARCH_SYSTEM = \"\"\"You are a research synthesizer for technical writing.\n",
    "\n",
    "Given raw web search results, produce a deduplicated list of EvidenceItem objects.\n",
    "\n",
    "Rules:\n",
    "- Only include items with a non-empty url.\n",
    "- Prefer relevant + authoritative sources (company blogs, docs, reputable outlets).\n",
    "- Extract/normalize published_at as ISO (YYYY-MM-DD) if you can infer it from title/snippet.\n",
    "  If you can't infer a date reliably, set published_at=null (do NOT guess).\n",
    "- Keep snippets short.\n",
    "- Deduplicate by URL.\n",
    "\"\"\"\n",
    "\n",
    "def research_node(state: State) -> dict:\n",
    "    queries = (state.get(\"queries\", []) or [])[:10]\n",
    "    max_results = 6\n",
    "\n",
    "    raw_results: List[dict] = []\n",
    "    for q in queries:\n",
    "        raw_results.extend(_tavily_search(q, max_results=max_results))\n",
    "\n",
    "    if not raw_results:\n",
    "        return {\"evidence\": []}\n",
    "\n",
    "    extractor = llm.with_structured_output(EvidencePack)\n",
    "    pack = extractor.invoke(\n",
    "        [\n",
    "            SystemMessage(content=RESEARCH_SYSTEM),\n",
    "            HumanMessage(\n",
    "                content=(\n",
    "                    f\"As-of date: {state['as_of']}\\n\"\n",
    "                    f\"Recency days: {state['recency_days']}\\n\\n\"\n",
    "                    f\"Raw results:\\n{raw_results}\"\n",
    "                )\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Deduplicate by URL\n",
    "    dedup = {}\n",
    "    for e in pack.evidence:\n",
    "        if e.url:\n",
    "            dedup[e.url] = e\n",
    "    evidence = list(dedup.values())\n",
    "\n",
    "    # HARD RECENCY FILTER for open_book weekly roundup:\n",
    "    # keep only items with a parseable ISO date and within the window.\n",
    "    mode = state.get(\"mode\", \"closed_book\")\n",
    "    if mode == \"open_book\":\n",
    "        as_of = date.fromisoformat(state[\"as_of\"])\n",
    "        cutoff = as_of - timedelta(days=int(state[\"recency_days\"]))\n",
    "        fresh: List[EvidenceItem] = []\n",
    "        for e in evidence:\n",
    "            d = _iso_to_date(e.published_at)\n",
    "            if d and d >= cutoff:\n",
    "                fresh.append(e)\n",
    "        evidence = fresh\n",
    "\n",
    "    return {\"evidence\": evidence}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c06809f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 5) Orchestrator (Plan)\n",
    "# -----------------------------\n",
    "ORCH_SYSTEM = \"\"\"You are a senior technical writer and developer advocate.\n",
    "Your job is to produce a highly actionable outline for a technical blog post.\n",
    "\n",
    "Hard requirements:\n",
    "- Create 5–9 sections (tasks) suitable for the topic and audience.\n",
    "- Each task must include:\n",
    "  1) goal (1 sentence)\n",
    "  2) 3–6 bullets that are concrete, specific, and non-overlapping\n",
    "  3) target word count (120–550)\n",
    "\n",
    "Flexibility:\n",
    "- Do NOT use a fixed taxonomy unless it naturally fits.\n",
    "- You may tag tasks (tags field), but tags are flexible.\n",
    "\n",
    "Quality bar:\n",
    "- Assume the reader is a developer; use correct terminology.\n",
    "- Bullets must be actionable: build/compare/measure/verify/debug.\n",
    "- Ensure the overall plan includes at least 2 of these somewhere:\n",
    "  * minimal code sketch / MWE (set requires_code=True for that section)\n",
    "  * edge cases / failure modes\n",
    "  * performance/cost considerations\n",
    "  * security/privacy considerations (if relevant)\n",
    "  * debugging/observability tips\n",
    "\n",
    "Grounding rules:\n",
    "- Mode closed_book: keep it evergreen; do not depend on evidence.\n",
    "- Mode hybrid:\n",
    "  - Use evidence for up-to-date examples (models/tools/releases) in bullets.\n",
    "  - Mark sections using fresh info as requires_research=True and requires_citations=True.\n",
    "- Mode open_book (weekly news roundup):\n",
    "  - Set blog_kind = \"news_roundup\".\n",
    "  - Every section is about summarizing events + implications.\n",
    "  - DO NOT include tutorial/how-to sections (no scraping/RSS/how to fetch news) unless user explicitly asked for that.\n",
    "  - If evidence is empty or insufficient, create a plan that transparently says \"insufficient fresh sources\"\n",
    "    and includes only what can be supported.\n",
    "\n",
    "Output must strictly match the Plan schema.\n",
    "\"\"\"\n",
    "\n",
    "def orchestrator_node(state: State) -> dict:\n",
    "    planner = llm.with_structured_output(Plan)\n",
    "    evidence = state.get(\"evidence\", [])\n",
    "    mode = state.get(\"mode\", \"closed_book\")\n",
    "\n",
    "    # Force blog_kind for open_book\n",
    "    forced_kind = \"news_roundup\" if mode == \"open_book\" else None\n",
    "\n",
    "    plan = planner.invoke(\n",
    "        [\n",
    "            SystemMessage(content=ORCH_SYSTEM),\n",
    "            HumanMessage(\n",
    "                content=(\n",
    "                    f\"Topic: {state['topic']}\\n\"\n",
    "                    f\"Mode: {mode}\\n\"\n",
    "                    f\"As-of: {state['as_of']} (recency_days={state['recency_days']})\\n\"\n",
    "                    f\"{'Force blog_kind=news_roundup' if forced_kind else ''}\\n\\n\"\n",
    "                    f\"Evidence (ONLY use for fresh claims; may be empty):\\n\"\n",
    "                    f\"{[e.model_dump() for e in evidence][:16]}\\n\\n\"\n",
    "                    f\"Instruction: If mode=open_book, your plan must NOT drift into a tutorial.\"\n",
    "                )\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Ensure open_book forces the kind even if model forgets\n",
    "    if forced_kind:\n",
    "        plan.blog_kind = \"news_roundup\"\n",
    "\n",
    "    return {\"plan\": plan}\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Fanout\n",
    "# -----------------------------\n",
    "def fanout(state: State):\n",
    "    assert state[\"plan\"] is not None\n",
    "    return [\n",
    "        Send(\n",
    "            \"worker\",\n",
    "            {\n",
    "                \"task\": task.model_dump(),\n",
    "                \"topic\": state[\"topic\"],\n",
    "                \"mode\": state[\"mode\"],\n",
    "                \"as_of\": state[\"as_of\"],\n",
    "                \"recency_days\": state[\"recency_days\"],\n",
    "                \"plan\": state[\"plan\"].model_dump(),\n",
    "                \"evidence\": [e.model_dump() for e in state.get(\"evidence\", [])],\n",
    "            },\n",
    "        )\n",
    "        for task in state[\"plan\"].tasks\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6f50a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 7) Worker (write one section)\n",
    "# -----------------------------\n",
    "WORKER_SYSTEM = \"\"\"You are a senior technical writer and developer advocate.\n",
    "Write ONE section of a technical blog post in Markdown.\n",
    "\n",
    "Hard constraints:\n",
    "- Follow the provided Goal and cover ALL Bullets in order (do not skip or merge bullets).\n",
    "- Stay close to Target words (±15%).\n",
    "- Output ONLY the section content in Markdown (no blog title H1, no extra commentary).\n",
    "- Start with a '## <Section Title>' heading.\n",
    "\n",
    "Scope guard (prevents mid-blog topic drift):\n",
    "- If blog_kind == \"news_roundup\": do NOT turn this into a tutorial/how-to guide.\n",
    "  Do NOT teach web scraping, RSS, automation, or \"how to fetch news\" unless bullets explicitly ask for it.\n",
    "  Focus on summarizing events and implications.\n",
    "\n",
    "Grounding policy:\n",
    "- If mode == open_book (weekly news):\n",
    "  - Do NOT introduce any specific event/company/model/funding/policy claim unless it is supported by provided Evidence URLs.\n",
    "  - For each event claim, attach a source as a Markdown link: ([Source](URL)).\n",
    "  - Only use URLs provided in Evidence. If not supported, write: \"Not found in provided sources.\"\n",
    "- If requires_citations == true (hybrid sections):\n",
    "  - For outside-world claims, cite Evidence URLs the same way.\n",
    "- Evergreen reasoning (concepts, intuition) is OK without citations unless requires_citations is true.\n",
    "\n",
    "Code:\n",
    "- If requires_code == true, include at least one minimal, correct code snippet relevant to the bullets.\n",
    "\n",
    "Style:\n",
    "- Short paragraphs, bullets where helpful, code fences for code.\n",
    "- Avoid fluff/marketing. Be precise and implementation-oriented.\n",
    "\"\"\"\n",
    "\n",
    "def worker_node(payload: dict) -> dict:\n",
    "    \n",
    "    task = Task(**payload[\"task\"])\n",
    "    plan = Plan(**payload[\"plan\"])\n",
    "    evidence = [EvidenceItem(**e) for e in payload.get(\"evidence\", [])]\n",
    "    topic = payload[\"topic\"]\n",
    "    mode = payload.get(\"mode\", \"closed_book\")\n",
    "    as_of = payload.get(\"as_of\")\n",
    "    recency_days = payload.get(\"recency_days\")\n",
    "\n",
    "    bullets_text = \"\\n- \" + \"\\n- \".join(task.bullets)\n",
    "\n",
    "    # Provide a compact evidence list for citation use\n",
    "    evidence_text = \"\"\n",
    "    if evidence:\n",
    "        evidence_text = \"\\n\".join(\n",
    "            f\"- {e.title} | {e.url} | {e.published_at or 'date:unknown'}\".strip()\n",
    "            for e in evidence[:20]\n",
    "        )\n",
    "\n",
    "    section_md = llm.invoke(\n",
    "        [\n",
    "            SystemMessage(content=WORKER_SYSTEM),\n",
    "            HumanMessage(\n",
    "                content=(\n",
    "                    f\"Blog title: {plan.blog_title}\\n\"\n",
    "                    f\"Audience: {plan.audience}\\n\"\n",
    "                    f\"Tone: {plan.tone}\\n\"\n",
    "                    f\"Blog kind: {plan.blog_kind}\\n\"\n",
    "                    f\"Constraints: {plan.constraints}\\n\"\n",
    "                    f\"Topic: {topic}\\n\"\n",
    "                    f\"Mode: {mode}\\n\"\n",
    "                    f\"As-of: {as_of} (recency_days={recency_days})\\n\\n\"\n",
    "                    f\"Section title: {task.title}\\n\"\n",
    "                    f\"Goal: {task.goal}\\n\"\n",
    "                    f\"Target words: {task.target_words}\\n\"\n",
    "                    f\"Tags: {task.tags}\\n\"\n",
    "                    f\"requires_research: {task.requires_research}\\n\"\n",
    "                    f\"requires_citations: {task.requires_citations}\\n\"\n",
    "                    f\"requires_code: {task.requires_code}\\n\"\n",
    "                    f\"Bullets:{bullets_text}\\n\\n\"\n",
    "                    f\"Evidence (ONLY use these URLs when citing):\\n{evidence_text}\\n\"\n",
    "                )\n",
    "            ),\n",
    "        ]\n",
    "    ).content.strip()\n",
    "\n",
    "    # deterministic ordering\n",
    "    return {\"sections\": [(task.id, section_md)]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1f45bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 8) Reducer (merge + save)\n",
    "# -----------------------------\n",
    "def reducer_node(state: State) -> dict:\n",
    "    plan = state[\"plan\"]\n",
    "    if plan is None:\n",
    "        raise ValueError(\"Reducer called without a plan.\")\n",
    "\n",
    "    ordered_sections = [md for _, md in sorted(state[\"sections\"], key=lambda x: x[0])]\n",
    "    body = \"\\n\\n\".join(ordered_sections).strip()\n",
    "    final_md = f\"# {plan.blog_title}\\n\\n{body}\\n\"\n",
    "\n",
    "    filename = f\"{plan.blog_title}.md\"\n",
    "    Path(filename).write_text(final_md, encoding=\"utf-8\")\n",
    "\n",
    "    return {\"final\": final_md}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adfb355c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x00000212A848B770>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 9) Build graph\n",
    "# -----------------------------\n",
    "g = StateGraph(State)\n",
    "g.add_node(\"router\", router_node)\n",
    "g.add_node(\"research\", research_node)\n",
    "g.add_node(\"orchestrator\", orchestrator_node)\n",
    "g.add_node(\"worker\", worker_node)\n",
    "g.add_node(\"reducer\", reducer_node)\n",
    "\n",
    "g.add_edge(START, \"router\")\n",
    "g.add_conditional_edges(\"router\", route_next, {\"research\": \"research\", \"orchestrator\": \"orchestrator\"})\n",
    "g.add_edge(\"research\", \"orchestrator\")\n",
    "\n",
    "g.add_conditional_edges(\"orchestrator\", fanout, [\"worker\"])\n",
    "g.add_edge(\"worker\", \"reducer\")\n",
    "g.add_edge(\"reducer\", END)\n",
    "\n",
    "app = g.compile()\n",
    "\n",
    "app\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "430f5bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 10) Runner\n",
    "# -----------------------------\n",
    "def run(topic: str, as_of: Optional[str] = None):\n",
    "    if as_of is None:\n",
    "        as_of = date.today().isoformat()\n",
    "\n",
    "    out = app.invoke(\n",
    "        {\n",
    "            \"topic\": topic,\n",
    "            \"mode\": \"\",\n",
    "            \"needs_research\": False,\n",
    "            \"queries\": [],\n",
    "            \"evidence\": [],\n",
    "            \"plan\": None,\n",
    "            \"as_of\": as_of,\n",
    "            \"recency_days\": 7,   # router may overwrite\n",
    "            \"sections\": [],\n",
    "            \"final\": \"\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    plan: Plan = out[\"plan\"]\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"TOPIC:\", topic)\n",
    "    print(\"AS_OF:\", out.get(\"as_of\"), \"RECENCY_DAYS:\", out.get(\"recency_days\"))\n",
    "    print(\"MODE:\", out.get(\"mode\"))\n",
    "    print(\"BLOG_KIND:\", plan.blog_kind)\n",
    "    print(\"NEEDS_RESEARCH:\", out.get(\"needs_research\"))\n",
    "    print(\"QUERIES:\", (out.get(\"queries\") or [])[:6])\n",
    "    print(\"EVIDENCE_COUNT:\", len(out.get(\"evidence\", [])))\n",
    "    if out.get(\"evidence\"):\n",
    "        print(\"EVIDENCE_SAMPLE:\", [e.model_dump() for e in out[\"evidence\"][:2]])\n",
    "    print(\"TASKS:\", len(plan.tasks))\n",
    "    print(\"SAVED_MD_CHARS:\", len(out.get(\"final\", \"\")))\n",
    "    print(\"=\" * 100 + \"\\n\")\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5be9f123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "TOPIC: Write a blog on Self Attention\n",
      "AS_OF: 2026-02-24 RECENCY_DAYS: 3650\n",
      "MODE: closed_book\n",
      "BLOG_KIND: explainer\n",
      "NEEDS_RESEARCH: False\n",
      "QUERIES: []\n",
      "EVIDENCE_COUNT: 0\n",
      "TASKS: 6\n",
      "SAVED_MD_CHARS: 18802\n",
      "====================================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'topic': 'Write a blog on Self Attention',\n",
       " 'mode': 'closed_book',\n",
       " 'needs_research': False,\n",
       " 'queries': [],\n",
       " 'evidence': [],\n",
       " 'plan': Plan(blog_title='Demystifying Self-Attention: Understanding the Core Mechanism Behind Modern Transformers', audience='Developers and machine learning engineers familiar with deep learning fundamentals', tone='Informative and practical', blog_kind='explainer', constraints=['Explain concepts in a developer-friendly manner', 'Include actionable insights and practical code sketches', 'Avoid excessive jargon', 'Ensure coverage of edge cases and debugging tips'], tasks=[Task(id=1, title='Introduction to Self-Attention', goal='Explain what self-attention is and why it is fundamental in modern deep learning architectures.', bullets=['Define self-attention and distinguish it from traditional attention mechanisms.', 'Explain the role of self-attention in enabling models to weigh relationships within a single sequence.', 'Briefly outline key historical context: transformer models and their impact on NLP and beyond.', 'Highlight the importance of self-attention in handling sequence data efficiently.', 'Summarize what the reader will gain in understanding this mechanism.', 'Provide clear, concise motivation for developers to learn self-attention.'], target_words=300, tags=['conceptual'], requires_research=False, requires_citations=False, requires_code=False), Task(id=2, title='Mathematical Foundations of Self-Attention', goal='Equip the reader with a precise mathematical understanding of how self-attention computes weighted representations of input sequences.', bullets=['Describe input representations: queries, keys, and values and how they are derived from inputs.', 'Explain the computation of attention scores using dot-product and why scaling is necessary.', 'Demonstrate the use of softmax to convert scores into attention probabilities.', 'Discuss how vectors are combined based on attention weights to produce output embeddings.', 'Clarify the role of multiple attention heads in capturing diverse relationships.', 'Include minimal equations and simple illustrative examples for intuition.'], target_words=450, tags=['technical'], requires_research=False, requires_citations=False, requires_code=False), Task(id=3, title='Implementing a Simple Self-Attention Module in PyTorch', goal='Guide the reader through building a minimal working example of a self-attention layer in PyTorch.', bullets=['Set up a basic input representation tensor suitable for batch processing.', 'Define learnable projection matrices for queries, keys, and values.', 'Write the forward pass computing attention scores, applying softmax, and generating output vectors.', 'Test the module with a sample input and verify output shapes.', 'Discuss best practices for efficient tensor operations, e.g., using batch matrix multiplications.', 'Outline how to extend this minimal module to multi-head attention.'], target_words=400, tags=['implementation', 'code'], requires_research=False, requires_citations=False, requires_code=True), Task(id=4, title='Performance Considerations and Scaling Challenges', goal='Explain the computational and memory challenges of self-attention and how they influence model deployment choices.', bullets=['Discuss the quadratic complexity in sequence length and its impact on large inputs.', 'Compare standard self-attention with sparse and approximate attention techniques used for efficiency.', 'Highlight trade-offs between accuracy and performance when applying attention approximations.', 'Introduce common methods to reduce latency and memory usage, including batching and pruning.', 'Provide tips on profiling and benchmarking self-attention components.', 'Explain implications for deploying models on resource-constrained environments.'], target_words=350, tags=['performance'], requires_research=False, requires_citations=False, requires_code=False), Task(id=5, title='Common Pitfalls and Debugging Strategies in Self-Attention Implementations', goal='Help developers identify and troubleshoot typical errors encountered when implementing or using self-attention layers.', bullets=['Highlight common mistakes such as shape mismatches in queries, keys, and values.', 'Discuss numerical stability issues like overflow in softmax and mitigation strategies (e.g. max subtraction).', 'Explain how to verify attention weights to ensure the model focuses as expected.', 'Introduce visualization techniques for attention maps to aid debugging.', 'Provide debugging tips related to gradient flow and learned parameter initialization.', 'Advise on using unit tests and logging intermediate tensor states.'], target_words=400, tags=['debugging', 'edge_cases'], requires_research=False, requires_citations=False, requires_code=False), Task(id=6, title='Extending Self-Attention: Beyond the Basics', goal='Introduce advanced self-attention variations and their applications to inspire further exploration.', bullets=['Summarize multi-head attention and how it captures different subspace representations.', 'Briefly discuss relative positional encoding and its significance in sequence modeling.', 'Mention cross-attention and its role in encoder-decoder architectures.', 'Outline recent innovations like efficient attention variants (e.g., Linformer, Performer).', 'Suggest resources and libraries for experimenting with advanced self-attention layers.'], target_words=300, tags=['conceptual', 'future'], requires_research=False, requires_citations=False, requires_code=False)]),\n",
       " 'as_of': '2026-02-24',\n",
       " 'recency_days': 3650,\n",
       " 'sections': [(1,\n",
       "   '## Introduction to Self-Attention\\n\\nSelf-attention is a mechanism that allows a model to evaluate and weigh the importance of different elements within the same input sequence. Unlike traditional attention mechanisms, which often focus on relating sequences across separate inputs—such as aligning words in translation tasks—self-attention operates intra-sequentially. This means it helps the model understand how each token in a sequence relates to every other token in that same sequence.\\n\\nThe core idea behind self-attention is to dynamically compute representations of input elements by considering their contextual relationships. For example, in a sentence, a word’s meaning often depends on other words around it. Self-attention captures these dependencies without relying on fixed-lookback windows, thus enabling the model to attend globally across the sequence.\\n\\nHistorically, self-attention gained prominence with the introduction of the Transformer model in 2017. Transformers leveraged self-attention to replace recurrent and convolutional architectures in natural language processing (NLP), achieving unprecedented performance on tasks like machine translation, text summarization, and later extending to vision and multimodal problems. This shift marked a fundamental change in how sequence data is processed: from sequential step-by-step reading to a parallelizable, scalable framework that can model long-range dependencies efficiently.\\n\\nFrom a developer’s perspective, understanding self-attention is crucial since it underpins many state-of-the-art models in not only NLP but also in areas like speech recognition, recommendation systems, and even code generation. Mastering this mechanism will enable you to design or debug models that better capture complex patterns in sequence data, optimize performance, and innovate on custom applications.\\n\\nIn this blog, you’ll gain a clear grasp of what self-attention does, why it matters, and how it functions at a high level. We will demystify the concept and provide practical insights that you can directly apply to deep learning workflows, paving the way for deeper exploration of transformer architectures and their modern adaptations.'),\n",
       "  (2,\n",
       "   '## Mathematical Foundations of Self-Attention\\n\\nAt the heart of self-attention lies the idea of relating different positions of an input sequence to compute a new representation for each element. To make this concrete, let\\'s break down the key mathematical components.\\n\\n### Queries, Keys, and Values\\n\\nGiven an input sequence represented by embeddings \\\\(X = [x_1, x_2, ..., x_n]\\\\), self-attention maps each element into three vectors: **query** \\\\(q_i\\\\), **key** \\\\(k_i\\\\), and **value** \\\\(v_i\\\\). These are computed via learned linear projections:\\n\\n\\\\[\\nq_i = W^Q x_i, \\\\quad k_i = W^K x_i, \\\\quad v_i = W^V x_i,\\n\\\\]\\n\\nwhere \\\\(W^Q, W^K, W^V\\\\) are parameter matrices. Intuitively:\\n\\n- **Queries** represent what the current element \"asks\" about the sequence.\\n- **Keys** encode what information each element \"contains.\"\\n- **Values** hold the actual information to be aggregated.\\n\\nThis triad allows every position to attend (focus) on the relevant parts of the sequence.\\n\\n### Computing Attention Scores\\n\\nThe relevance of each element \\\\(j\\\\) to the query at position \\\\(i\\\\) is evaluated via a dot-product between query and key:\\n\\n\\\\[\\n\\\\text{score}(i, j) = q_i \\\\cdot k_j = q_i^{\\\\top} k_j.\\n\\\\]\\n\\nThis captures similarity: higher dot-product implies more relevance.\\n\\nHowever, since these vectors often have high dimensionality \\\\(d_k\\\\), the dot products can grow large in magnitude, leading to very small gradients when passed through softmax. To mitigate this, the scores are scaled by \\\\(\\\\frac{1}{\\\\sqrt{d_k}}\\\\):\\n\\n\\\\[\\n\\\\text{scaled\\\\_score}(i, j) = \\\\frac{q_i^{\\\\top} k_j}{\\\\sqrt{d_k}}.\\n\\\\]\\n\\nThis scaling ensures stable gradients and more balanced attention distributions.\\n\\n### From Scores to Attention Weights\\n\\nNext, the scaled scores for query \\\\(i\\\\) over all positions \\\\(j=1..n\\\\) are normalized into probabilities using the softmax function:\\n\\n\\\\[\\n\\\\alpha_{ij} = \\\\frac{\\\\exp \\\\left(\\\\frac{q_i^{\\\\top} k_j}{\\\\sqrt{d_k}} \\\\right)}{\\\\sum_{m=1}^n \\\\exp \\\\left(\\\\frac{q_i^{\\\\top} k_m}{\\\\sqrt{d_k}}\\\\right)}.\\n\\\\]\\n\\nHere, \\\\(\\\\alpha_{ij}\\\\) represents the attention weight that position \\\\(i\\\\) places on position \\\\(j\\\\).\\n\\n### Forming the Output\\n\\nThe output at position \\\\(i\\\\) is the weighted sum of all value vectors, aggregated according to the attention weights:\\n\\n\\\\[\\nz_i = \\\\sum_{j=1}^n \\\\alpha_{ij} v_j.\\n\\\\]\\n\\nThus, the new representation \\\\(z_i\\\\) incorporates context from positions deemed relevant.\\n\\n### Multiple Attention Heads\\n\\nTo capture diverse relationships and subspaces, self-attention uses multiple heads. Each head performs independent projections:\\n\\n\\\\[\\n\\\\text{head}_h = \\\\text{Attention}(X W_h^Q, X W_h^K, X W_h^V),\\n\\\\]\\n\\nwith distinct parameter sets \\\\((W_h^Q, W_h^K, W_h^V)\\\\). The outputs from all heads are concatenated and linearly transformed to form the final output.\\n\\nThis multi-headed setup allows the model to attend to information at different granularities and semantic aspects simultaneously.\\n\\n### Illustrative Example\\n\\nConsider a sequence with three tokens and embedding dimension \\\\(d=4\\\\):\\n\\n\\\\[\\nX = \\\\begin{bmatrix}\\n1 & 0 & 1 & 0 \\\\\\\\\\n0 & 2 & 0 & 2 \\\\\\\\\\n1 & 1 & 1 & 1\\n\\\\end{bmatrix}.\\n\\\\]\\n\\nA single head with projection matrices \\\\(W^Q, W^K, W^V \\\\in \\\\mathbb{R}^{4 \\\\times 2}\\\\) reduces dimensionality to 2. After computing queries and keys, suppose:\\n\\n\\\\[\\nq_1 = [1, 0], \\\\quad k_1 = [1, 0], \\\\quad k_2 = [0, 1], \\\\quad k_3 = [1, 1].\\n\\\\]\\n\\nThe score of token 1 attending to token 3 is:\\n\\n\\\\[\\n\\\\text{score}(1,3) = q_1 \\\\cdot k_3 = 1 \\\\times 1 + 0 \\\\times 1 = 1.\\n\\\\]\\n\\nAssuming \\\\(d_k=2\\\\), the scaled score is \\\\(\\\\frac{1}{\\\\sqrt{2}} \\\\approx 0.707\\\\).\\n\\nApplying softmax to scores \\\\([ \\\\frac{q_1 \\\\cdot k_1}{\\\\sqrt{2}}, \\\\frac{q_1 \\\\cdot k_2}{\\\\sqrt{2}}, 0.707]\\\\) yields attention weights \\\\(\\\\alpha\\\\). These weights then sum the corresponding values to produce \\\\(z_1\\\\).\\n\\n---\\n\\nUnderstanding these mathematical steps clarifies not only *how* self-attention computes contextual embeddings but also *why* design choices like scaling and multi-head attention are essential for stability and expressiveness. When implementing, careful attention to numerical stability (e.g., subtracting max scores before softmax) and verifying shapes during projection can help debug common issues.'),\n",
       "  (3,\n",
       "   \"## Implementing a Simple Self-Attention Module in PyTorch\\n\\nLet's build a minimal self-attention layer step-by-step in PyTorch to clarify its core mechanics and provide a reusable module.\\n\\n### Setting up the input tensor\\n\\nSelf-attention operates on sequences in batch. We'll represent inputs as a 3D tensor of shape `(batch_size, seq_len, embedding_dim)`. For example, a batch with 2 sequences, each length 4, embedding dimension 8:\\n\\n```python\\nimport torch\\n\\nbatch_size, seq_len, embed_dim = 2, 4, 8\\nx = torch.randn(batch_size, seq_len, embed_dim)\\n```\\n\\n### Defining learnable projections\\n\\nSelf-attention transforms inputs into queries, keys, and values via linear projections. We create three `nn.Linear` layers to learn these:\\n\\n```python\\nimport torch.nn as nn\\n\\nclass SimpleSelfAttention(nn.Module):\\n    def __init__(self, embed_dim):\\n        super().__init__()\\n        self.query_proj = nn.Linear(embed_dim, embed_dim)\\n        self.key_proj = nn.Linear(embed_dim, embed_dim)\\n        self.value_proj = nn.Linear(embed_dim, embed_dim)\\n        \\n    def forward(self, x):\\n        # We'll implement this next\\n        pass\\n```\\n\\n### Forward pass: computing attention scores and output\\n\\nInside `forward`, we project inputs, compute scaled dot-product attention, and use softmax to get weights:\\n\\n```python\\ndef forward(self, x):\\n    Q = self.query_proj(x)  # (batch_size, seq_len, embed_dim)\\n    K = self.key_proj(x)    # (batch_size, seq_len, embed_dim)\\n    V = self.value_proj(x)  # (batch_size, seq_len, embed_dim)\\n\\n    # Compute attention scores with batch matmul; transpose K for dot product\\n    scores = torch.bmm(Q, K.transpose(1, 2))  # (batch_size, seq_len, seq_len)\\n\\n    # Scale scores by sqrt of embed_dim to stabilize gradients\\n    scale = embed_dim ** 0.5\\n    scaled_scores = scores / scale\\n\\n    # Softmax over the key dimension to get attention weights\\n    attn_weights = torch.softmax(scaled_scores, dim=-1)  # (batch_size, seq_len, seq_len)\\n\\n    # Weighted sum of values\\n    output = torch.bmm(attn_weights, V)  # (batch_size, seq_len, embed_dim)\\n    return output\\n```\\n\\nAdd this method to your `SimpleSelfAttention` class.\\n\\n### Testing and verifying output shapes\\n\\nInstantiate the module and pass the example input:\\n\\n```python\\nmodel = SimpleSelfAttention(embed_dim)\\nout = model(x)\\nprint(out.shape)  # Expected: (2, 4, 8)\\n```\\n\\nThe output shape matches the input’s `(batch_size, seq_len, embed_dim)`, confirming the self-attention layer preserves the sequence structure.\\n\\n### Best practices for efficiency\\n\\n- Use `torch.bmm` (batch matrix multiplication) instead of explicit Python loops—this leverages GPU parallelism.\\n- Scale attention scores by the square root of embedding dimension to prevent softmax saturation.\\n- Avoid in-place operations on tensors involved in gradient computation.\\n- For debugging, verify shapes at each step to catch any broadcasting issues early.\\n\\n### Extending to multi-head attention\\n\\nMulti-head attention splits embeddings into multiple heads, running attention parallelly to capture diverse relationships:\\n\\n- Divide `embed_dim` into `num_heads` subspaces.\\n- Use separate projections per head or a combined projection followed by reshaping.\\n- Compute scaled dot-product attention independently for each head.\\n- Concatenate heads’ outputs and project back to original `embed_dim`.\\n\\nThis modular design lays the foundation to build more complex transformer blocks efficiently.\"),\n",
       "  (4,\n",
       "   '## Performance Considerations and Scaling Challenges\\n\\nSelf-attention is a powerful mechanism but comes with notable computational and memory costs, especially as input sequence length grows. At its core, self-attention involves calculating pairwise interactions between all tokens in a sequence, resulting in a *quadratic* complexity in sequence length (O(n²)). This means that as the input size doubles, the required compute and memory increase roughly fourfold, posing serious challenges for long sequences such as lengthy documents or high-resolution images.\\n\\nTo address these scaling issues, researchers and engineers have developed alternatives to standard self-attention. **Sparse attention** techniques selectively compute interactions only between certain tokens—like nearby neighbors or fixed patterns—to reduce the number of computations. **Approximate attention** methods use low-rank factorizations or kernel-based approaches to estimate attention scores more efficiently. These methods can lower complexity closer to linear with sequence length in some cases, enabling the processing of much longer inputs.\\n\\nHowever, these optimizations come with trade-offs. Approximations can degrade accuracy or contextual understanding since they bypass some token relationships. Choosing the right balance depends on the application’s tolerance for errors and the performance budget. For instance, a latency-sensitive system might prioritize speed over slight drops in accuracy, while a research prototype might accept higher costs for maximal fidelity.\\n\\nBeyond choosing attention variants, practical steps to reduce latency and memory usage include:\\n\\n- **Batching** inputs smartly to maximize hardware utilization without exceeding memory limits.\\n- **Pruning** less important attention heads or tokens after training to slim down models.\\n- Using mixed-precision arithmetic to reduce memory consumption without large numerical errors.\\n\\nProfiling tools like NVIDIA Nsight, PyTorch’s autograd profiler, or TensorFlow’s profiler are invaluable for identifying bottlenecks in the attention computation. Benchmarking different implementations under realistic conditions ensures you understand scaling behavior and resource demands.\\n\\nWhen deploying models on resource-constrained environments—such as edge devices or mobile—these considerations become critical. Limited CPU/GPU memory and throughput often necessitate using efficient attention approximations, model compression, or splitting inputs into manageable chunks while maintaining acceptable performance.\\n\\nIn summary, understanding and mitigating self-attention’s quadratic overhead is key for scalable and efficient transformer deployments. Careful method selection, profiling, and resource-aware engineering unlock practical use even for long sequences and constrained hardware.'),\n",
       "  (5,\n",
       "   '## Common Pitfalls and Debugging Strategies in Self-Attention Implementations\\n\\nImplementing self-attention layers can be challenging, especially when subtle bugs or numerical issues arise. Below are common mistakes and practical strategies to help you debug and ensure your self-attention module functions correctly.\\n\\n### Shape Mismatches in Queries, Keys, and Values\\n\\nA frequent source of errors is incompatible tensor shapes. In self-attention, queries (Q), keys (K), and values (V) must have aligned dimensions to perform dot-product operations correctly:\\n\\n- Ensure Q and K have compatible shapes for the dot product, usually `(batch_size, num_heads, seq_len, head_dim)`.\\n- Make sure V matches the shape `(batch_size, num_heads, seq_len, head_dim)` to enable weighted summation.\\n- Use explicit assertions or print statements to verify tensor shapes before and after each operation.\\n\\nNeglecting these checks often causes runtime errors or silent shape broadcasting issues that degrade model accuracy.\\n\\n### Numerical Stability in Softmax\\n\\nThe softmax step computing attention weights can suffer from numerical overflow or underflow, especially with large dot-product values. To mitigate this:\\n\\n- Always subtract the maximum value per query vector (`scores -= scores.max(dim=-1, keepdim=True)`) before applying softmax. This normalization improves numerical stability without changing the output probabilities.\\n- Double-check that the softmax dimension is correct, typically along the sequence length axis.\\n- Consider using stable PyTorch or TensorFlow built-in softmax implementations which often incorporate these tricks internally.\\n\\n### Verifying Attention Weights\\n\\nTo confirm the self-attention mechanism is functioning as intended:\\n\\n- Verify that attention weights sum to 1 along the sequence length dimension using assertions.\\n- Check for pathological cases where attention distribution collapses to a single token or becomes uniform; either may indicate issues.\\n- Print or log statistics such as mean or entropy of attention weights for quick sanity checks.\\n\\n### Visualization Techniques for Attention Maps\\n\\nVisualizing attention maps is invaluable for debugging:\\n\\n- Plot heatmaps of attention weights for sample inputs to see where the model focuses.\\n- Use color-coded matrix plots to highlight tokens that receive high attention.\\n- If using multi-head attention, inspect different heads individually to detect any head collapse or redundancy.\\n\\nThese visual clues can quickly reveal problems like misaligned positional encodings or dataset peculiarities.\\n\\n### Debugging Gradient Flow and Parameter Initialization\\n\\nGradient vanishing or exploding can halt learning:\\n\\n- Monitor gradients flowing into Q, K, V projection layers to ensure they are neither zero nor excessively large.\\n- Use gradient clipping if sudden spikes occur during backpropagation.\\n- Initialize projection weights with standard methods (e.g., Xavier or Kaiming) to promote stable training.\\n- Beware of biases misalignments that can skew attention scores early in training.\\n\\n### Unit Tests and Logging Intermediate Tensors\\n\\nFinally, robust debugging relies on:\\n\\n- Writing unit tests for self-attention modules that check output shapes, attention weight normalization, and behavior on synthetic inputs.\\n- Logging intermediate tensor states (e.g., QK scores, softmax outputs) in development to detect anomalies early.\\n- Automating tests for boundary cases such as empty sequences or single-token inputs.\\n\\nBy combining careful shape validation, numerical stability tactics, visualization, and rigorous testing, you can confidently implement and maintain efficient, reliable self-attention layers.'),\n",
       "  (6,\n",
       "   '## Extending Self-Attention: Beyond the Basics\\n\\nBuilding on the core self-attention mechanism, several advanced variations have been developed to enhance model expressiveness and efficiency.\\n\\n**Multi-head attention** splits the attention mechanism into multiple parallel \"heads,\" each learning to focus on different representation subspaces. This allows the model to capture diverse contextual information simultaneously, enriching feature extraction compared to a single attention head. Each head attends to a complementary aspect of the input, enabling the model to combine varied perspectives before producing the final representation.\\n\\n**Relative positional encoding** addresses a limitation of absolute positional embeddings by encoding the relative distances between tokens. This is crucial in tasks where the relationship between positions matters more than their absolute index, such as in language or protein sequences. By incorporating relative positions directly into the attention scores, models gain improved generalization to varying sequence lengths and better capture structural dependencies.\\n\\n**Cross-attention** extends self-attention to scenarios with multiple input streams, playing a central role in encoder-decoder architectures like transformers used in machine translation and image captioning. Here, the decoder queries the encoder\\'s output via cross-attention, effectively aligning and conditioning on relevant encoded information while generating sequences.\\n\\nRecent research has introduced **efficient attention variants** to tackle the quadratic complexity of self-attention with long sequences. Models like **Linformer** approximate the full attention matrix using low-rank projections, drastically reducing computation and memory while maintaining performance. Similarly, **Performer** leverages kernel methods to approximate attention with linear complexity, enabling scalability to very long inputs.\\n\\nFor developers eager to experiment, libraries such as Hugging Face\\'s Transformers, TensorFlow Addons, and PyTorch\\'s Transformer modules include implementations of multi-head and cross-attention layers, often supporting extensions with relative positional encodings. Exploring these tools alongside new efficient attention variants can deepen understanding and accelerate prototyping of advanced models.\\n\\nBy grasping these extended self-attention concepts, you open doors to building more powerful and efficient models tailored to complex sequence-processing challenges.')],\n",
       " 'final': '# Demystifying Self-Attention: Understanding the Core Mechanism Behind Modern Transformers\\n\\n## Introduction to Self-Attention\\n\\nSelf-attention is a mechanism that allows a model to evaluate and weigh the importance of different elements within the same input sequence. Unlike traditional attention mechanisms, which often focus on relating sequences across separate inputs—such as aligning words in translation tasks—self-attention operates intra-sequentially. This means it helps the model understand how each token in a sequence relates to every other token in that same sequence.\\n\\nThe core idea behind self-attention is to dynamically compute representations of input elements by considering their contextual relationships. For example, in a sentence, a word’s meaning often depends on other words around it. Self-attention captures these dependencies without relying on fixed-lookback windows, thus enabling the model to attend globally across the sequence.\\n\\nHistorically, self-attention gained prominence with the introduction of the Transformer model in 2017. Transformers leveraged self-attention to replace recurrent and convolutional architectures in natural language processing (NLP), achieving unprecedented performance on tasks like machine translation, text summarization, and later extending to vision and multimodal problems. This shift marked a fundamental change in how sequence data is processed: from sequential step-by-step reading to a parallelizable, scalable framework that can model long-range dependencies efficiently.\\n\\nFrom a developer’s perspective, understanding self-attention is crucial since it underpins many state-of-the-art models in not only NLP but also in areas like speech recognition, recommendation systems, and even code generation. Mastering this mechanism will enable you to design or debug models that better capture complex patterns in sequence data, optimize performance, and innovate on custom applications.\\n\\nIn this blog, you’ll gain a clear grasp of what self-attention does, why it matters, and how it functions at a high level. We will demystify the concept and provide practical insights that you can directly apply to deep learning workflows, paving the way for deeper exploration of transformer architectures and their modern adaptations.\\n\\n## Mathematical Foundations of Self-Attention\\n\\nAt the heart of self-attention lies the idea of relating different positions of an input sequence to compute a new representation for each element. To make this concrete, let\\'s break down the key mathematical components.\\n\\n### Queries, Keys, and Values\\n\\nGiven an input sequence represented by embeddings \\\\(X = [x_1, x_2, ..., x_n]\\\\), self-attention maps each element into three vectors: **query** \\\\(q_i\\\\), **key** \\\\(k_i\\\\), and **value** \\\\(v_i\\\\). These are computed via learned linear projections:\\n\\n\\\\[\\nq_i = W^Q x_i, \\\\quad k_i = W^K x_i, \\\\quad v_i = W^V x_i,\\n\\\\]\\n\\nwhere \\\\(W^Q, W^K, W^V\\\\) are parameter matrices. Intuitively:\\n\\n- **Queries** represent what the current element \"asks\" about the sequence.\\n- **Keys** encode what information each element \"contains.\"\\n- **Values** hold the actual information to be aggregated.\\n\\nThis triad allows every position to attend (focus) on the relevant parts of the sequence.\\n\\n### Computing Attention Scores\\n\\nThe relevance of each element \\\\(j\\\\) to the query at position \\\\(i\\\\) is evaluated via a dot-product between query and key:\\n\\n\\\\[\\n\\\\text{score}(i, j) = q_i \\\\cdot k_j = q_i^{\\\\top} k_j.\\n\\\\]\\n\\nThis captures similarity: higher dot-product implies more relevance.\\n\\nHowever, since these vectors often have high dimensionality \\\\(d_k\\\\), the dot products can grow large in magnitude, leading to very small gradients when passed through softmax. To mitigate this, the scores are scaled by \\\\(\\\\frac{1}{\\\\sqrt{d_k}}\\\\):\\n\\n\\\\[\\n\\\\text{scaled\\\\_score}(i, j) = \\\\frac{q_i^{\\\\top} k_j}{\\\\sqrt{d_k}}.\\n\\\\]\\n\\nThis scaling ensures stable gradients and more balanced attention distributions.\\n\\n### From Scores to Attention Weights\\n\\nNext, the scaled scores for query \\\\(i\\\\) over all positions \\\\(j=1..n\\\\) are normalized into probabilities using the softmax function:\\n\\n\\\\[\\n\\\\alpha_{ij} = \\\\frac{\\\\exp \\\\left(\\\\frac{q_i^{\\\\top} k_j}{\\\\sqrt{d_k}} \\\\right)}{\\\\sum_{m=1}^n \\\\exp \\\\left(\\\\frac{q_i^{\\\\top} k_m}{\\\\sqrt{d_k}}\\\\right)}.\\n\\\\]\\n\\nHere, \\\\(\\\\alpha_{ij}\\\\) represents the attention weight that position \\\\(i\\\\) places on position \\\\(j\\\\).\\n\\n### Forming the Output\\n\\nThe output at position \\\\(i\\\\) is the weighted sum of all value vectors, aggregated according to the attention weights:\\n\\n\\\\[\\nz_i = \\\\sum_{j=1}^n \\\\alpha_{ij} v_j.\\n\\\\]\\n\\nThus, the new representation \\\\(z_i\\\\) incorporates context from positions deemed relevant.\\n\\n### Multiple Attention Heads\\n\\nTo capture diverse relationships and subspaces, self-attention uses multiple heads. Each head performs independent projections:\\n\\n\\\\[\\n\\\\text{head}_h = \\\\text{Attention}(X W_h^Q, X W_h^K, X W_h^V),\\n\\\\]\\n\\nwith distinct parameter sets \\\\((W_h^Q, W_h^K, W_h^V)\\\\). The outputs from all heads are concatenated and linearly transformed to form the final output.\\n\\nThis multi-headed setup allows the model to attend to information at different granularities and semantic aspects simultaneously.\\n\\n### Illustrative Example\\n\\nConsider a sequence with three tokens and embedding dimension \\\\(d=4\\\\):\\n\\n\\\\[\\nX = \\\\begin{bmatrix}\\n1 & 0 & 1 & 0 \\\\\\\\\\n0 & 2 & 0 & 2 \\\\\\\\\\n1 & 1 & 1 & 1\\n\\\\end{bmatrix}.\\n\\\\]\\n\\nA single head with projection matrices \\\\(W^Q, W^K, W^V \\\\in \\\\mathbb{R}^{4 \\\\times 2}\\\\) reduces dimensionality to 2. After computing queries and keys, suppose:\\n\\n\\\\[\\nq_1 = [1, 0], \\\\quad k_1 = [1, 0], \\\\quad k_2 = [0, 1], \\\\quad k_3 = [1, 1].\\n\\\\]\\n\\nThe score of token 1 attending to token 3 is:\\n\\n\\\\[\\n\\\\text{score}(1,3) = q_1 \\\\cdot k_3 = 1 \\\\times 1 + 0 \\\\times 1 = 1.\\n\\\\]\\n\\nAssuming \\\\(d_k=2\\\\), the scaled score is \\\\(\\\\frac{1}{\\\\sqrt{2}} \\\\approx 0.707\\\\).\\n\\nApplying softmax to scores \\\\([ \\\\frac{q_1 \\\\cdot k_1}{\\\\sqrt{2}}, \\\\frac{q_1 \\\\cdot k_2}{\\\\sqrt{2}}, 0.707]\\\\) yields attention weights \\\\(\\\\alpha\\\\). These weights then sum the corresponding values to produce \\\\(z_1\\\\).\\n\\n---\\n\\nUnderstanding these mathematical steps clarifies not only *how* self-attention computes contextual embeddings but also *why* design choices like scaling and multi-head attention are essential for stability and expressiveness. When implementing, careful attention to numerical stability (e.g., subtracting max scores before softmax) and verifying shapes during projection can help debug common issues.\\n\\n## Implementing a Simple Self-Attention Module in PyTorch\\n\\nLet\\'s build a minimal self-attention layer step-by-step in PyTorch to clarify its core mechanics and provide a reusable module.\\n\\n### Setting up the input tensor\\n\\nSelf-attention operates on sequences in batch. We\\'ll represent inputs as a 3D tensor of shape `(batch_size, seq_len, embedding_dim)`. For example, a batch with 2 sequences, each length 4, embedding dimension 8:\\n\\n```python\\nimport torch\\n\\nbatch_size, seq_len, embed_dim = 2, 4, 8\\nx = torch.randn(batch_size, seq_len, embed_dim)\\n```\\n\\n### Defining learnable projections\\n\\nSelf-attention transforms inputs into queries, keys, and values via linear projections. We create three `nn.Linear` layers to learn these:\\n\\n```python\\nimport torch.nn as nn\\n\\nclass SimpleSelfAttention(nn.Module):\\n    def __init__(self, embed_dim):\\n        super().__init__()\\n        self.query_proj = nn.Linear(embed_dim, embed_dim)\\n        self.key_proj = nn.Linear(embed_dim, embed_dim)\\n        self.value_proj = nn.Linear(embed_dim, embed_dim)\\n        \\n    def forward(self, x):\\n        # We\\'ll implement this next\\n        pass\\n```\\n\\n### Forward pass: computing attention scores and output\\n\\nInside `forward`, we project inputs, compute scaled dot-product attention, and use softmax to get weights:\\n\\n```python\\ndef forward(self, x):\\n    Q = self.query_proj(x)  # (batch_size, seq_len, embed_dim)\\n    K = self.key_proj(x)    # (batch_size, seq_len, embed_dim)\\n    V = self.value_proj(x)  # (batch_size, seq_len, embed_dim)\\n\\n    # Compute attention scores with batch matmul; transpose K for dot product\\n    scores = torch.bmm(Q, K.transpose(1, 2))  # (batch_size, seq_len, seq_len)\\n\\n    # Scale scores by sqrt of embed_dim to stabilize gradients\\n    scale = embed_dim ** 0.5\\n    scaled_scores = scores / scale\\n\\n    # Softmax over the key dimension to get attention weights\\n    attn_weights = torch.softmax(scaled_scores, dim=-1)  # (batch_size, seq_len, seq_len)\\n\\n    # Weighted sum of values\\n    output = torch.bmm(attn_weights, V)  # (batch_size, seq_len, embed_dim)\\n    return output\\n```\\n\\nAdd this method to your `SimpleSelfAttention` class.\\n\\n### Testing and verifying output shapes\\n\\nInstantiate the module and pass the example input:\\n\\n```python\\nmodel = SimpleSelfAttention(embed_dim)\\nout = model(x)\\nprint(out.shape)  # Expected: (2, 4, 8)\\n```\\n\\nThe output shape matches the input’s `(batch_size, seq_len, embed_dim)`, confirming the self-attention layer preserves the sequence structure.\\n\\n### Best practices for efficiency\\n\\n- Use `torch.bmm` (batch matrix multiplication) instead of explicit Python loops—this leverages GPU parallelism.\\n- Scale attention scores by the square root of embedding dimension to prevent softmax saturation.\\n- Avoid in-place operations on tensors involved in gradient computation.\\n- For debugging, verify shapes at each step to catch any broadcasting issues early.\\n\\n### Extending to multi-head attention\\n\\nMulti-head attention splits embeddings into multiple heads, running attention parallelly to capture diverse relationships:\\n\\n- Divide `embed_dim` into `num_heads` subspaces.\\n- Use separate projections per head or a combined projection followed by reshaping.\\n- Compute scaled dot-product attention independently for each head.\\n- Concatenate heads’ outputs and project back to original `embed_dim`.\\n\\nThis modular design lays the foundation to build more complex transformer blocks efficiently.\\n\\n## Performance Considerations and Scaling Challenges\\n\\nSelf-attention is a powerful mechanism but comes with notable computational and memory costs, especially as input sequence length grows. At its core, self-attention involves calculating pairwise interactions between all tokens in a sequence, resulting in a *quadratic* complexity in sequence length (O(n²)). This means that as the input size doubles, the required compute and memory increase roughly fourfold, posing serious challenges for long sequences such as lengthy documents or high-resolution images.\\n\\nTo address these scaling issues, researchers and engineers have developed alternatives to standard self-attention. **Sparse attention** techniques selectively compute interactions only between certain tokens—like nearby neighbors or fixed patterns—to reduce the number of computations. **Approximate attention** methods use low-rank factorizations or kernel-based approaches to estimate attention scores more efficiently. These methods can lower complexity closer to linear with sequence length in some cases, enabling the processing of much longer inputs.\\n\\nHowever, these optimizations come with trade-offs. Approximations can degrade accuracy or contextual understanding since they bypass some token relationships. Choosing the right balance depends on the application’s tolerance for errors and the performance budget. For instance, a latency-sensitive system might prioritize speed over slight drops in accuracy, while a research prototype might accept higher costs for maximal fidelity.\\n\\nBeyond choosing attention variants, practical steps to reduce latency and memory usage include:\\n\\n- **Batching** inputs smartly to maximize hardware utilization without exceeding memory limits.\\n- **Pruning** less important attention heads or tokens after training to slim down models.\\n- Using mixed-precision arithmetic to reduce memory consumption without large numerical errors.\\n\\nProfiling tools like NVIDIA Nsight, PyTorch’s autograd profiler, or TensorFlow’s profiler are invaluable for identifying bottlenecks in the attention computation. Benchmarking different implementations under realistic conditions ensures you understand scaling behavior and resource demands.\\n\\nWhen deploying models on resource-constrained environments—such as edge devices or mobile—these considerations become critical. Limited CPU/GPU memory and throughput often necessitate using efficient attention approximations, model compression, or splitting inputs into manageable chunks while maintaining acceptable performance.\\n\\nIn summary, understanding and mitigating self-attention’s quadratic overhead is key for scalable and efficient transformer deployments. Careful method selection, profiling, and resource-aware engineering unlock practical use even for long sequences and constrained hardware.\\n\\n## Common Pitfalls and Debugging Strategies in Self-Attention Implementations\\n\\nImplementing self-attention layers can be challenging, especially when subtle bugs or numerical issues arise. Below are common mistakes and practical strategies to help you debug and ensure your self-attention module functions correctly.\\n\\n### Shape Mismatches in Queries, Keys, and Values\\n\\nA frequent source of errors is incompatible tensor shapes. In self-attention, queries (Q), keys (K), and values (V) must have aligned dimensions to perform dot-product operations correctly:\\n\\n- Ensure Q and K have compatible shapes for the dot product, usually `(batch_size, num_heads, seq_len, head_dim)`.\\n- Make sure V matches the shape `(batch_size, num_heads, seq_len, head_dim)` to enable weighted summation.\\n- Use explicit assertions or print statements to verify tensor shapes before and after each operation.\\n\\nNeglecting these checks often causes runtime errors or silent shape broadcasting issues that degrade model accuracy.\\n\\n### Numerical Stability in Softmax\\n\\nThe softmax step computing attention weights can suffer from numerical overflow or underflow, especially with large dot-product values. To mitigate this:\\n\\n- Always subtract the maximum value per query vector (`scores -= scores.max(dim=-1, keepdim=True)`) before applying softmax. This normalization improves numerical stability without changing the output probabilities.\\n- Double-check that the softmax dimension is correct, typically along the sequence length axis.\\n- Consider using stable PyTorch or TensorFlow built-in softmax implementations which often incorporate these tricks internally.\\n\\n### Verifying Attention Weights\\n\\nTo confirm the self-attention mechanism is functioning as intended:\\n\\n- Verify that attention weights sum to 1 along the sequence length dimension using assertions.\\n- Check for pathological cases where attention distribution collapses to a single token or becomes uniform; either may indicate issues.\\n- Print or log statistics such as mean or entropy of attention weights for quick sanity checks.\\n\\n### Visualization Techniques for Attention Maps\\n\\nVisualizing attention maps is invaluable for debugging:\\n\\n- Plot heatmaps of attention weights for sample inputs to see where the model focuses.\\n- Use color-coded matrix plots to highlight tokens that receive high attention.\\n- If using multi-head attention, inspect different heads individually to detect any head collapse or redundancy.\\n\\nThese visual clues can quickly reveal problems like misaligned positional encodings or dataset peculiarities.\\n\\n### Debugging Gradient Flow and Parameter Initialization\\n\\nGradient vanishing or exploding can halt learning:\\n\\n- Monitor gradients flowing into Q, K, V projection layers to ensure they are neither zero nor excessively large.\\n- Use gradient clipping if sudden spikes occur during backpropagation.\\n- Initialize projection weights with standard methods (e.g., Xavier or Kaiming) to promote stable training.\\n- Beware of biases misalignments that can skew attention scores early in training.\\n\\n### Unit Tests and Logging Intermediate Tensors\\n\\nFinally, robust debugging relies on:\\n\\n- Writing unit tests for self-attention modules that check output shapes, attention weight normalization, and behavior on synthetic inputs.\\n- Logging intermediate tensor states (e.g., QK scores, softmax outputs) in development to detect anomalies early.\\n- Automating tests for boundary cases such as empty sequences or single-token inputs.\\n\\nBy combining careful shape validation, numerical stability tactics, visualization, and rigorous testing, you can confidently implement and maintain efficient, reliable self-attention layers.\\n\\n## Extending Self-Attention: Beyond the Basics\\n\\nBuilding on the core self-attention mechanism, several advanced variations have been developed to enhance model expressiveness and efficiency.\\n\\n**Multi-head attention** splits the attention mechanism into multiple parallel \"heads,\" each learning to focus on different representation subspaces. This allows the model to capture diverse contextual information simultaneously, enriching feature extraction compared to a single attention head. Each head attends to a complementary aspect of the input, enabling the model to combine varied perspectives before producing the final representation.\\n\\n**Relative positional encoding** addresses a limitation of absolute positional embeddings by encoding the relative distances between tokens. This is crucial in tasks where the relationship between positions matters more than their absolute index, such as in language or protein sequences. By incorporating relative positions directly into the attention scores, models gain improved generalization to varying sequence lengths and better capture structural dependencies.\\n\\n**Cross-attention** extends self-attention to scenarios with multiple input streams, playing a central role in encoder-decoder architectures like transformers used in machine translation and image captioning. Here, the decoder queries the encoder\\'s output via cross-attention, effectively aligning and conditioning on relevant encoded information while generating sequences.\\n\\nRecent research has introduced **efficient attention variants** to tackle the quadratic complexity of self-attention with long sequences. Models like **Linformer** approximate the full attention matrix using low-rank projections, drastically reducing computation and memory while maintaining performance. Similarly, **Performer** leverages kernel methods to approximate attention with linear complexity, enabling scalability to very long inputs.\\n\\nFor developers eager to experiment, libraries such as Hugging Face\\'s Transformers, TensorFlow Addons, and PyTorch\\'s Transformer modules include implementations of multi-head and cross-attention layers, often supporting extensions with relative positional encodings. Exploring these tools alongside new efficient attention variants can deepen understanding and accelerate prototyping of advanced models.\\n\\nBy grasping these extended self-attention concepts, you open doors to building more powerful and efficient models tailored to complex sequence-processing challenges.\\n'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run(\"Write a blog on Self Attention\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18b4d07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraphenv (3.13.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
